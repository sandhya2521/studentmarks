# -*- coding: utf-8 -*-
"""Copy of student_marks_prediction_project_Elewayte.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_q6_fbBJHYl24BXk4fCJke_h1e8G3Jj8

---

---

# <center> ★ AI / ML Project - Student Marks Prediction ★
#### <center> ***Domain: Education***

---

---

### Description:

The data consists of Marks of students including their study time & number of courses. The dataset is downloaded from UCI Machine Learning Repository.

**Properties of the Dataset:** \
Number of Instances: 100\
Number of Attributes: 3 including the target variable.

The project is simple yet challenging as it is has very limited features & samples. Can you build regression model to capture all the patterns in the dataset, also maitaining the generalisability of the model?


### Objective:
- Understand the Dataset & cleanup (if required).
- Build Regression models to predict the student marks wrt multiple features.
- Also evaluate the models & compare their respective scores like R2, RMSE, etc.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



data = pd.read_csv("Student_Marks.csv")
data.head()

data.shape

data.isnull().sum()

plt.scatter(x = data.time_study , y=data.Marks)
plt.title("Student Data")
plt.xlabel("time_study")
plt.ylabel("Marks")
plt.show()

data.mean()

data = data.fillna(data.mean())
data.isnull().sum()

X = data.drop(columns = 'time_study')
y = data.drop(columns = 'Marks')
X.shape , y.shape

from sklearn.model_selection import train_test_split

X_train , X_test , y_train , y_test = train_test_split(X, y , random_state=51 , test_size=0.2)
X_train.shape , y_train.shape , X_test.shape , y_test.shape

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train , y_train)

lr.score(X_test , y_test)

lr.intercept_

plt.scatter(X_train, y_train)
plt.plot(X_train ,lr.predict(X_train) , color='r')

import joblib

joblib.dump(lr , 'Student_Marks_Prediction_Model.pkl')

model = joblib.load('Student_Marks_Prediction_Model.pkl')

data.describe()

"""# **PLAN OF ACTION**


**We aim to solve the problem statement by creating a plan of action, Here are some of the necessary steps:**
1. Data Exploration
2. Exploratory Data Analysis (EDA)
3. Data Pre-processing
4. Data Manipulation
5. Feature Selection/Extraction
6. Predictive Modelling
7. Project Outcomes & Conclusion

# <center>1. Data Exploration
"""

# import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# load the dataset into a pandas dataframe

df = pd.read_csv("student_marks.csv")  # use your dataset filename instead of student_marks.csv

# check for missing values
print(df.isnull().sum())

# check data types
print(df.dtypes)

# describe summary statistics of the data
print(df.describe())

# visualize the distribution of each feature
# !!!!NOTE!!!!: include all the column names by using sns.displot(df['column_name']) )
sns.displot(df['column_name'])
sns.displot(df['column_name'])
sns.displot(df['column_name'])
sns.displot(df['column_name'])
# etc etc 



# visualize the relationship between the features and the target variable
sns.pairplot(df)

# compute correlation matrix
corr_matrix = df.corr()

# visualize the correlation matrix as a heatmap
sns.heatmap(corr_matrix, annot=True)

"""# <center> 2. Exploratory Data Analysis (EDA)"""

# check the distribution of each feature using histograms

# !!!!NOTE!!!!: include all the column names for the histograms using axs[0].hist(df['column_names']) and also set title for the same using axs[0].set_title('column_name2') DO IT FOR ALL COLUMNS

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
axs[0].hist(df['column_name1'])
axs[0].set_title('column_name1')
axs[1].hist(df['column_name2'])
axs[1].set_title('column_name2')
axs[2].hist(df['column_name3'])
axs[2].set_title('column_name4')
# etc etc 
plt.show()

# compute pairwise correlation between features and target variable
corr_matrix = df.corr()
print(corr_matrix)

# visualize correlation matrix using heatmap
sns.heatmap(corr_matrix, annot=True, cmap="YlGnBu")
plt.show()

# visualize scatterplots between features and target variable
# !!! NOTE !!! : include all the columns names list below and change y_vars value to the name of the column name related to marks
sns.pairplot(df, x_vars=['coumn_name1', 'column_name2', 'etc'], y_vars='column_name_related to marks', kind='scatter')
plt.show()

"""# <center> 3. Data Preprocessing"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# separate features and target variable
# !!! NOTE !!! : change 'column_name_related to mark' to the column name related to marks from the dataset
X = df.drop('column_name_related to mark', axis=1)
y = df['column_name_related to mark']

# standardize features using StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# <center> 4. Data Manipulation"""

# create a new feature that represents the product of study time and number of courses
# !!! NOTE !!! : change 'product', 'study_time', 'number_of_courses'according  to the column names related to the same from the dataset (make changes everywhere)
df['product'] = df['study_time'] * df['number_of_courses']

# drop the original features that were used to create the new feature
df = df.drop(['study_time', 'number_of_courses'], axis=1)

# save the manipulated data to a new CSV file
df.to_csv('manipulated_student_marks.csv', index=False)

"""FOLLOW THE REMAINING STEPS AS PER THE INSTRUCTIONS

# <center> 5. Feature Selection/Extraction
"""

from sklearn.feature_selection import SelectKBest, f_regression

# use SelectKBest to select top 1 feature based on f_regression
selector = SelectKBest(score_func=f_regression, k=1)
X_new = selector.fit_transform(X, y)

# print the selected feature
selected_feature_index = selector.get_support(indices=True)[0]
selected_feature_name = X.columns[selected_feature_index]
print("Selected feature:", selected_feature_name)

"""# <center> 6.Predictive Modelling"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# create a linear regression model
model = LinearRegression()

# fit the model on the training data
model.fit(X_train, y_train)

# make predictions on the test data
y_pred = model.predict(X_test)

# evaluate the model using r2_score and mean_squared_error
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

# print the evaluation metrics
print("R2 score:", r2)
print("Root Mean Squared Error:", rmse)

"""# <center>7. Project Outcomes & Conclusion

ADD THE R2 SCORE AND RMSE SCORE AND PROVIDE THE CONCLUSION!!
"""